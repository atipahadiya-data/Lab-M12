{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cba7b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source orders created:\n",
      "   order_id  order_date customer   product  amount\n",
      "0         1  2024-06-01    Alice  Widget A    75.0\n",
      "1         2  2024-06-01      Bob  Widget B    50.0\n",
      "2         3  2024-06-02  Charlie  Widget A   150.0\n",
      "3         4  2024-06-02    Diana  Widget C    30.0\n",
      "4         5  2024-06-03    Alice  Widget B   100.0\n",
      "5         6  2024-06-03      Bob  Widget A    75.0\n",
      "6         7  2024-06-04      Eve  Widget C    60.0\n",
      "7         8  2024-06-04  Charlie  Widget B    50.0\n",
      "8         9  2024-06-05    Diana  Widget A   150.0\n",
      "9        10  2024-06-05    Alice  Widget C    90.0\n",
      "\n",
      "Total rows: 10\n",
      "Order ID range: 1 to 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create source data (simulating a production database table)\n",
    "data = {\n",
    "    \"order_id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"order_date\": [\n",
    "        \"2024-06-01\", \"2024-06-01\", \"2024-06-02\", \"2024-06-02\", \"2024-06-03\",\n",
    "        \"2024-06-03\", \"2024-06-04\", \"2024-06-04\", \"2024-06-05\", \"2024-06-05\"\n",
    "    ],\n",
    "    \"customer\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Alice\",\n",
    "                 \"Bob\", \"Eve\", \"Charlie\", \"Diana\", \"Alice\"],\n",
    "    \"product\": [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget B\",\n",
    "                \"Widget A\", \"Widget C\", \"Widget B\", \"Widget A\", \"Widget C\"],\n",
    "    \"amount\": [75.00, 50.00, 150.00, 30.00, 100.00, \n",
    "               75.00, 60.00, 50.00, 150.00, 90.00]\n",
    "}\n",
    "\n",
    "source_df = pd.DataFrame(data)\n",
    "source_df.to_csv(\"source_orders.csv\", index=False)\n",
    "\n",
    "print(\"Source orders created:\")\n",
    "print(source_df)\n",
    "print(f\"\\nTotal rows: {len(source_df)}\")\n",
    "print(f\"Order ID range: {source_df['order_id'].min()} to {source_df['order_id'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915dc06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty warehouse file created.\n"
     ]
    }
   ],
   "source": [
    "# create an empty target wharehouse file :\n",
    "pd.DataFrame(columns=source_df.columns).to_csv(\"warehouse_orders.csv\", index=False)\n",
    "print(\"Empty warehouse file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51074e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create functions to read and write the checkpoint.\n",
    "CHECKPOINT_FILE = \"checkpoint.json\"\n",
    "\n",
    "def read_checkpoint():\n",
    "    \"\"\"\n",
    "    Read the last processed order_id from the checkpoint file.\n",
    "    Returns 0 if no checkpoint exists (first run).\n",
    "    \"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            checkpoint = json.load(f)\n",
    "        print(f\"Checkpoint found: last_processed_id = {checkpoint['last_processed_id']}\")\n",
    "        return checkpoint[\"last_processed_id\"]\n",
    "    else:\n",
    "        print(\"No checkpoint found — this is the FIRST RUN\")\n",
    "        return 0  # No checkpoint = start from beginning\n",
    "\n",
    "def write_checkpoint(last_processed_id):\n",
    "    \"\"\"\n",
    "    Save the last processed order_id to the checkpoint file.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        \"last_processed_id\": last_processed_id,\n",
    "        \"updated_at\": pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump(checkpoint, f, indent=2)\n",
    "    print(f\"Checkpoint updated: last_processed_id = {last_processed_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277b875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found — this is the FIRST RUN\n",
      "Starting from order_id > 0\n",
      "Checkpoint updated: last_processed_id = 5\n",
      "Checkpoint found: last_processed_id = 5\n",
      "After writing 5, checkpoint returns: 5\n",
      "Checkpoint removed for fresh start\n"
     ]
    }
   ],
   "source": [
    "# test checkpoint functions\n",
    "\n",
    "# Test: no checkpoint exists yet\n",
    "last_id = read_checkpoint()\n",
    "print(f\"Starting from order_id > {last_id}\")\n",
    "\n",
    "# Test: write and read back\n",
    "write_checkpoint(5)\n",
    "last_id = read_checkpoint()\n",
    "print(f\"After writing 5, checkpoint returns: {last_id}\")\n",
    "\n",
    "# Clean up for next steps\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "    print(\"Checkpoint removed for fresh start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81314d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def incremental_load(source_file, warehouse_file):\n",
    "    \"\"\"\n",
    "    Load only NEW rows from source since the last checkpoint.\n",
    "    \n",
    "    Steps:\n",
    "    1. Read checkpoint (last processed ID)\n",
    "    2. Read source and filter rows > checkpoint\n",
    "    3. Append new rows to warehouse\n",
    "    4. Update checkpoint to max ID loaded\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 55}\")\n",
    "    print(f\"  INCREMENTAL LOAD\")\n",
    "    print(f\"{'=' * 55}\")\n",
    "    \n",
    "    # Step 1: Read checkpoint\n",
    "    last_processed_id = read_checkpoint()\n",
    "    \n",
    "    # Step 2: Read source and filter new rows\n",
    "    source_df = pd.read_csv(source_file)\n",
    "    new_rows = source_df[source_df[\"order_id\"] > last_processed_id]\n",
    "    \n",
    "    print(f\"\\nSource total rows: {len(source_df)}\")\n",
    "    print(f\"New rows (order_id > {last_processed_id}): {len(new_rows)}\")\n",
    "    \n",
    "    # Step 3: Handle no new data\n",
    "    if len(new_rows) == 0:\n",
    "        print(\"No new data to load. Skipping.\")\n",
    "        return pd.read_csv(warehouse_file) if os.path.exists(warehouse_file) else pd.DataFrame()\n",
    "    \n",
    "    # Step 4: Read current warehouse and append\n",
    "    if os.path.exists(warehouse_file) and os.path.getsize(warehouse_file) > 0:\n",
    "        warehouse_df = pd.read_csv(warehouse_file)\n",
    "    else:\n",
    "        warehouse_df = pd.DataFrame(columns=source_df.columns)\n",
    "    \n",
    "    print(f\"Warehouse rows BEFORE: {len(warehouse_df)}\")\n",
    "    \n",
    "    warehouse_df = pd.concat([warehouse_df, new_rows], ignore_index=True)\n",
    "    warehouse_df.to_csv(warehouse_file, index=False)\n",
    "    \n",
    "    print(f\"Warehouse rows AFTER: {len(warehouse_df)}\")\n",
    "    \n",
    "    # Step 5: Update checkpoint\n",
    "    new_max_id = int(new_rows[\"order_id\"].max())\n",
    "    write_checkpoint(new_max_id)\n",
    "    \n",
    "    print(f\"\\n--- Incremental Load Summary ---\")\n",
    "    print(f\"New rows loaded: {len(new_rows)}\")\n",
    "    print(f\"New checkpoint: {new_max_id}\")\n",
    "    \n",
    "    return warehouse_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726e3d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean state: no checkpoint, empty warehouse\n",
      "\n",
      "===== FIRST RUN (no checkpoint) =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "No checkpoint found — this is the FIRST RUN\n",
      "\n",
      "Source total rows: 10\n",
      "New rows (order_id > 0): 10\n",
      "Warehouse rows BEFORE: 0\n",
      "Warehouse rows AFTER: 10\n",
      "Checkpoint updated: last_processed_id = 10\n",
      "\n",
      "--- Incremental Load Summary ---\n",
      "New rows loaded: 10\n",
      "New checkpoint: 10\n",
      "\n",
      "Warehouse contents:\n",
      "  order_id  order_date customer   product amount\n",
      "0        1  2024-06-01    Alice  Widget A   75.0\n",
      "1        2  2024-06-01      Bob  Widget B   50.0\n",
      "2        3  2024-06-02  Charlie  Widget A  150.0\n",
      "3        4  2024-06-02    Diana  Widget C   30.0\n",
      "4        5  2024-06-03    Alice  Widget B  100.0\n",
      "5        6  2024-06-03      Bob  Widget A   75.0\n",
      "6        7  2024-06-04      Eve  Widget C   60.0\n",
      "7        8  2024-06-04  Charlie  Widget B   50.0\n",
      "8        9  2024-06-05    Diana  Widget A  150.0\n",
      "9       10  2024-06-05    Alice  Widget C   90.0\n"
     ]
    }
   ],
   "source": [
    "# steps for running the incrementqal loader \n",
    "\n",
    "# Clean state\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    os.remove(CHECKPOINT_FILE)\n",
    "pd.DataFrame(columns=[\"order_id\",\"order_date\",\"customer\",\"product\",\"amount\"]).to_csv(\"warehouse_orders.csv\", index=False)\n",
    "print(\"Clean state: no checkpoint, empty warehouse\\n\")\n",
    "\n",
    "\n",
    "print(\"===== FIRST RUN (no checkpoint) =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(f\"\\nWarehouse contents:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c3f9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source now has 13 rows (added 3 new orders: IDs 11-13)\n"
     ]
    }
   ],
   "source": [
    "# simulate new data and run again \n",
    "# add new rows to the source \n",
    "\n",
    "# Simulate new orders arriving in the source\n",
    "new_orders = pd.DataFrame({\n",
    "    \"order_id\": [11, 12, 13],\n",
    "    \"order_date\": [\"2024-06-06\", \"2024-06-06\", \"2024-06-07\"],\n",
    "    \"customer\": [\"Frank\", \"Alice\", \"Bob\"],\n",
    "    \"product\": [\"Widget A\", \"Widget B\", \"Widget C\"],\n",
    "    \"amount\": [200.00, 75.00, 45.00]\n",
    "})\n",
    "\n",
    "# Append to source file\n",
    "source_df = pd.read_csv(\"source_orders.csv\")\n",
    "source_df = pd.concat([source_df, new_orders], ignore_index=True)\n",
    "source_df.to_csv(\"source_orders.csv\", index=False)\n",
    "\n",
    "print(f\"Source now has {len(source_df)} rows (added 3 new orders: IDs 11-13)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e0ac2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SECOND RUN (checkpoint exists) =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "Checkpoint found: last_processed_id = 10\n",
      "\n",
      "Source total rows: 13\n",
      "New rows (order_id > 10): 3\n",
      "Warehouse rows BEFORE: 10\n",
      "Warehouse rows AFTER: 13\n",
      "Checkpoint updated: last_processed_id = 13\n",
      "\n",
      "--- Incremental Load Summary ---\n",
      "New rows loaded: 3\n",
      "New checkpoint: 13\n",
      "\n",
      "Warehouse contents (last 5 rows):\n",
      "    order_id  order_date customer   product  amount\n",
      "8          9  2024-06-05    Diana  Widget A   150.0\n",
      "9         10  2024-06-05    Alice  Widget C    90.0\n",
      "10        11  2024-06-06    Frank  Widget A   200.0\n",
      "11        12  2024-06-06    Alice  Widget B    75.0\n",
      "12        13  2024-06-07      Bob  Widget C    45.0\n",
      "\n",
      "Total warehouse rows: 13\n"
     ]
    }
   ],
   "source": [
    "# run incremental load again \n",
    "\n",
    "print(\"\\n===== SECOND RUN (checkpoint exists) =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(f\"\\nWarehouse contents (last 5 rows):\")\n",
    "print(result.tail(5))\n",
    "print(f\"\\nTotal warehouse rows: {len(result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa5f7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== THIRD RUN (no new data) =====\n",
      "\n",
      "=======================================================\n",
      "  INCREMENTAL LOAD\n",
      "=======================================================\n",
      "Checkpoint found: last_processed_id = 13\n",
      "\n",
      "Source total rows: 13\n",
      "New rows (order_id > 13): 0\n",
      "No new data to load. Skipping.\n",
      "\n",
      "Total warehouse rows: 13\n",
      "Expected: No new data loaded, warehouse unchanged\n"
     ]
    }
   ],
   "source": [
    "# Test Edge Case — No New Data\n",
    "# Run again with no new data to verify it handles empty results.\n",
    "\n",
    "print(\"\\n===== THIRD RUN (no new data) =====\")\n",
    "result = incremental_load(\"source_orders.csv\", \"warehouse_orders.csv\")\n",
    "print(f\"\\nTotal warehouse rows: {len(result)}\")\n",
    "print(\"Expected: No new data loaded, warehouse unchanged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0695f49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FULL LOAD vs INCREMENTAL COMPARISON =====\n",
      "Full load: 13 rows processed in 0.0050s\n",
      "Checkpoint found: last_processed_id = 13\n",
      "Incremental: 0 rows to process in 0.0022s\n",
      "\n",
      "With real data (millions of rows), incremental saves significant time and cost!\n"
     ]
    }
   ],
   "source": [
    "# Compare Full Load vs Incremental\n",
    "import time\n",
    "\n",
    "print(\"\\n===== FULL LOAD vs INCREMENTAL COMPARISON =====\")\n",
    "\n",
    "# Full load: reads ALL source data every time\n",
    "start = time.time()\n",
    "full_df = pd.read_csv(\"source_orders.csv\")\n",
    "full_time = time.time() - start\n",
    "print(f\"Full load: {len(full_df)} rows processed in {full_time:.4f}s\")\n",
    "\n",
    "# Incremental: reads checkpoint, filters, loads only new\n",
    "start = time.time()\n",
    "last_id = read_checkpoint()\n",
    "source_df = pd.read_csv(\"source_orders.csv\")\n",
    "new_rows = source_df[source_df[\"order_id\"] > last_id]\n",
    "inc_time = time.time() - start\n",
    "print(f\"Incremental: {len(new_rows)} rows to process in {inc_time:.4f}s\")\n",
    "\n",
    "print(f\"\\nWith real data (millions of rows), incremental saves significant time and cost!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
